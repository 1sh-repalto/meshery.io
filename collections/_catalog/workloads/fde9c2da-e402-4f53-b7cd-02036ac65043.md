---
layout: item
name: ZooKeeper Cluster
userId: b6467d0a-f11e-4614-9eb2-ab80c4f1c58d
userName: Mohd  Uzair
userAvatarURL: https://lh3.googleusercontent.com/a/default-user=s96-c
type: Workloads
compatibility: 
        - KUBERNETES
patternId: fde9c2da-e402-4f53-b7cd-02036ac65043
image: https://raw.githubusercontent.com/layer5labs/meshery-extensions-packages/master/action-assets/design-assets/fde9c2da-e402-4f53-b7cd-02036ac65043.png
patternInfo: |
  This StatefulSet will create three Pods, each running a ZooKeeper server container. The Pods will be named my-zookeeper-cluster-0, my-zookeeper-cluster-1, and my-zookeeper-cluster-2. The volumeMounts section of the spec tells the Pods to mount the PersistentVolumeClaim my-zookeeper-cluster-pvc to the /zookeeper/data directory. This will ensure that the ZooKeeper data is persistent and stored across restarts.
patternCaveats: |
  1. The storage for a given Pod must either be provisioned by a PersistentVolume Provisioner based on the requested storage class, or pre-provisioned by an admin.
  2. Deleting and/or scaling a StatefulSet down will not delete the volumes associated with the StatefulSet. This is done to ensure data safety, which is generally more valuable than an automatic purge of all related StatefulSet resources.
  3. StatefulSets currently require a Headless Service to be responsible for the network identity of the   Pods. You are responsible for creating this Service.
  4. StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion.
  5. When using Rolling Updates with the default Pod Management Policy (OrderedReady), it's possible to get into a broken state that requires manual intervention to repair.
URL: 'https://raw.githubusercontent.com/meshery/meshery.io/master/catalog/fde9c2da-e402-4f53-b7cd-02036ac65043.yaml'
downloadLink: fde9c2da-e402-4f53-b7cd-02036ac65043.yaml
---
